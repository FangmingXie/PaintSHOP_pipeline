
# final pipeline endpoint
rule all:
    input:
        'pipeline_output/DONE.txt'

BOWTIE2_DIR =    f'pipeline_output/01_reference_files/06_bowtie2_indices/{config["assembly"]}'
JELLYFISH_IDX =  f"pipeline_output/01_reference_files/07_jf_files/{config['assembly']}.jf"
ANNOT_BED =      f'pipeline_output/01_reference_files/05_annotation_files/{config["assembly"]}.bed'
ANNOT_FLAT_BED = f'pipeline_output/01_reference_files/05_annotation_files/{config["assembly"]}_iso_flat.bed' 

checkpoint split_chroms:
    input:
        multi_fasta=f'pipeline_output/01_reference_files/02_multi_fasta/{config["assembly"]}.fa',
        chrom_names=f'pipeline_output/01_reference_files/01_chrom_names/chrom_names.txt',
    output:
        chrom_fasta_dir=directory('pipeline_output/01_reference_files/03_chrom_fastas'),
    conda:
        'envs/environment.yml'
    params:
        mfree='30G',
        h_rt='3:0:0'
    script:
        'scripts/split_chroms.py'

# align probes to reference genome
rule align_probes:
    input:
        'pipeline_output/02_intermediate_files/02_init_probes/{chrom}.fastq',
            f'{BOWTIE2_DIR}/{config["assembly"]}.1.bt2',
            f'{BOWTIE2_DIR}/{config["assembly"]}.2.bt2',
            f'{BOWTIE2_DIR}/{config["assembly"]}.3.bt2',
            f'{BOWTIE2_DIR}/{config["assembly"]}.4.bt2',
            f'{BOWTIE2_DIR}/{config["assembly"]}.rev.1.bt2',
            f'{BOWTIE2_DIR}/{config["assembly"]}.rev.2.bt2',
    output:
        'pipeline_output/02_intermediate_files/03_alignments/{chrom}.bam'
    conda:
        'envs/environment.yml'
    params:
        mfree='20G',
        h_rt='5:0:0'
    threads: config.get('bowtie2_threads', 4)
    shell:
        f'bowtie2 -x {BOWTIE2_DIR}/{config["assembly"]} -U {{input[0]}} --threads {{threads}} --very-sensitive-local -k 100 | '
        'samtools view -bS - > {output}'


# extract duplex information from alignment results
rule sam_to_pairwise:
    input:
        rules.align_probes.output
    output:
        'pipeline_output/02_intermediate_files/04_pairwise_alignments/{chrom}_pairwise.out'
    conda:
        'envs/environment.yml'
    params:
        mfree='12G',
        h_rt='3:0:0'
    shell:
        'samtools view {input} | sam2pairwise > {output}'


# extract alignment scores from pairwise alignment
rule extract_alignment_scores:
    input:
        rules.align_probes.output
    output:
        'pipeline_output/02_intermediate_files/05_alignment_scores/{chrom}_AS.txt'
    conda:
        'envs/environment.yml'
    params:
        mfree='12G',
        h_rt='3:0:0'
    shell:
        "samtools view {input} | awk '{{print $12}}' > {output}"


# construct alignment dataframe using pairwise alignment info
rule parse_pairwise:
    input:
        rules.sam_to_pairwise.output,
        rules.extract_alignment_scores.output
    output:
        'pipeline_output/02_intermediate_files/06_data_frames/{chrom}_alignments.csv'
    conda:
        'envs/environment.yml'
    params:
        mfree='85G',
        h_rt='3:0:0'
    script:
        'scripts/parse_pairwise.py'


# predict duplexing probabilities using pre-trained XGBoost model
rule predict_duplex:
    input:
        rules.parse_pairwise.output,
        f'{workflow.basedir}/pickled_models/{config.get("model_temp", 42)}_all_fixed_xgb.pickle.dat'
    output:
        'pipeline_output/02_intermediate_files/07_predictions/{chrom}_predictions.csv'
    conda:
        'envs/environment.yml'
    params:
        mfree='30G',
        h_rt='3:0:0'
    script:
        'scripts/XGB_predict.py'


# score probes based on XGBoost output
rule score:
    input:
        rules.predict_duplex.output
    output:
        'pipeline_output/02_intermediate_files/08_scored_probes/{chrom}_probes.bed'
    conda:
        'envs/environment.yml'
    params:
        mfree='20G',
        h_rt='3:0:0'
    script:
        'scripts/output_bed.py'


# remove any probes that contain "N" (sometimes introduced via bowtie2/sam2pairwise)
rule filter_n_probes:
    input:
        rules.score.output
    output:
        'pipeline_output/02_intermediate_files/09_probes_filtered/{chrom}_probes.bed'    
    conda:
        'envs/environment.yml'
    params:
        mfree='12G',
        h_rt='3:0:0'
    shell:
        "awk '$4 !~ /N/ && $4 !~ /-/' {input} > {output}"


# determine max kmer frequency using jellyfish
rule max_kmer:
    input:
        rules.filter_n_probes.output,
        JELLYFISH_IDX,
    output:
        'pipeline_output/02_intermediate_files/10_max_kmer_counts/{chrom}_kmer_max.txt'
    conda:
        'envs/environment.yml'
    params:
        mfree='20G',
        h_rt='3:0:0'
    script:
        'scripts/kmer_frequency.py'

# calculate prob value using nupack
rule calc_prob:
    input:
        rules.filter_n_probes.output,
    output:
        'pipeline_output/02_intermediate_files/11_prob_values/{chrom}_prob.txt'
    conda:
        'envs/environment.yml'
    params:
        mfree='12G',
        h_rt='6:0:0'
    script:
        'scripts/calc_prob.py'

# output final probe set
rule final_probes:
    input:
        rules.calc_prob.output,
        rules.max_kmer.output
    output:
        f'pipeline_output/03_output_files/01_dna_probes/{config["assembly"]}_all_newBalance_{{chrom}}.tsv'
    conda:
        'envs/environment.yml'
    params:
        mfree='12G',
        h_rt='3:0:0'
    script:
        'scripts/append_max_kmers.py'


# aggregate paths to dynamically created files
def get_probe_paths(wildcards):

    # construct wild card path to checkpoint output files
    wildcard_path = os.path.join(
        checkpoints.split_chroms.get(**wildcards).output[0], 
        '{chrom}.fa'
    )

    # construct paths to downstream files to be aggregated
    file_paths = expand(
        rules.final_probes.output[0],
        chrom=glob_wildcards(wildcard_path).chrom
    )

    # success
    return(file_paths)


# merge probes for each chromosome
rule merge_probes:
    input:
        get_probe_paths
    output:
        f'pipeline_output/03_output_files/01_dna_probes/{config["assembly"]}_all_newBalance.tsv',
    conda:
        'envs/environment.yml'
    params:
        mfree='20G',
        h_rt='3:0:0'
    shell:
        'for file in {input}; do cat $file >> {output}; done'


# zip probe files for each chromosome for PaintSHOP resources upload
rule zip_probes:
    input:
        rules.merge_probes.output
    output:
        f'pipeline_output/03_output_files/04_zip_archives/{config["assembly"]}_all_newBalance.zip'
    conda:
        'envs/environment.yml'
    params:
        mfree='10G',
        h_rt='3:0:0'
    shell:
        'zip -r -j {output} {input}'


# generate probe database using bedtools intersect
rule gen_refseq_db:
    input:
        probes=get_probe_paths,
        annotations=ANNOT_BED,
    output:
        tsv=f'pipeline_output/03_output_files/02_rna_probes_all/{config["assembly"]}_refseq_newBalance.tsv'
    conda:
        'envs/environment.yml'
    params:
        mfree='20G',
        h_rt='3:0:0'
    script:
        'scripts/probeDB.py'


# zip probe files for each chromosome for PaintSHOP resources upload
rule zip_refseq_db:
    input:
        rules.gen_refseq_db.output.tsv
    output:
        f'pipeline_output/03_output_files/04_zip_archives/{config["assembly"]}_refseq_newBalance.zip'
    conda:
        'envs/environment.yml'
    params:
        mfree='10G',
        h_rt='3:0:0'
    shell:
        'zip -r -j {output} {input}'


# generate probe database using bedtools intersect
rule gen_iso_db:
    input:
        probes=get_probe_paths,
        annotations=ANNOT_FLAT_BED, 

    output:
        tsv=f'pipeline_output/03_output_files/03_rna_probes_iso/{config["assembly"]}_iso_newBalance.tsv'
    conda:
        'envs/environment.yml'
    params:
        mfree='20G',
        h_rt='3:0:0'
    script:
        'scripts/probeDB.py'


# zip probe files for each chromosome for PaintSHOP resources upload
rule zip_iso_db:
    input:
        rules.gen_iso_db.output.tsv
    output:
        f'pipeline_output/03_output_files/04_zip_archives/{config["assembly"]}_iso_newBalance.zip'
    conda:
        'envs/environment.yml'
    params:
        mfree='10G',
        h_rt='3:0:0'
    shell:
        'zip -r -j {output} {input}'


# success
rule finish:
    input:
        rules.zip_probes.output,
        rules.zip_refseq_db.output,
        rules.zip_iso_db.output,
    output:
        'pipeline_output/DONE.txt'
    params:
        mfree='10G',
        h_rt='3:0:0'
    shell:
        'touch {output}'
